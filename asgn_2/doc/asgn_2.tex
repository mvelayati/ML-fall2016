
% CS534 implementation assignment 2

\documentclass{article}


\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[linesnumbered, ruled]{algorithm2e}
\SetKwRepeat{Do}{do}{while}%
\usepackage{titlesec}

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\vec}[1]{\mathbf{#1}}
%\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\titleformat{\section}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{10}{12}\bfseries}{\thesubsection}{1em}{}

\begin{document}

\begin{center}
	{\huge CS-534 Machine Learning} \\ \vspace{2mm}
	{\Large Implementation Assignment 2} \\ \vspace{2mm} \hrule \vspace{3mm}
	{\normalsize Mohammand Velayati ($33.\bar{3}$\%) \hspace{3mm}  Lucas Wells ($33.\bar{3}$\%) \hspace{3mm}  Chirag Shah ($33.\bar{3}$\%)} 
\end{center}

\section{Introduction}

In this assignment we are given a set of tweets from Hillary Clinton's and Donald Trump's Twitter accounts. We will train a Na\"{i}ve Bayes classifier to predict the account that a new tweet comes from. Throughout this document we use the variable $C$ to represent the account that a given tweet comes from, Hillary's or Trump's, and $\vec{x}$ represents a tweet, such that each $x_i$ is a single word in the tweet $\vec{x}$ of class $C$. Also, let $V$ be the vocabulary of unique words found in all tweets of the training set.

\subsection{The Na\"{i}ve Bayes classifier}

The Na\"{i}ve Bayes classifier is based on Bayes' Theorem,

\begin{align}
p(C|\vec{x}) = \frac{p(C)p(\vec{x}|C)}{p(\vec{x})} \propto p(C)p(\vec{x}|C),
\end{align}

\noindent
that states that the probability of of observing a class $C$, given a set of features $\vec{x}$, is equal to the prior probability of observing class $C$ times the likelihood of the feature begin in that class divided by the evidence $p(\vec{x})$. As the denominator does not depend on the class we can state that the probability of observing a class is proportional to the numerator. This is equivalent to the joint probability $p(C, x_1, \dots, x_n)$. In practice this computation on high dimensional features is prohibitive since we must estimate $k \times (2^d - 1)$ parameters. Na\"{i}ve Bayes relaxes the conditional probability model and assumes that each feature is conditionally independent from all other features. Thus, we can write the conditional distribution over $C$ as


\begin{align}
p(C|x_1, \dots ,x_n) \propto p(C)\prod^n_{i=1} p(x_i|C).
\end{align}

\noindent
Combining the Na\"{i}ve Bayes probability model with the Maximum A Posteriori (MAP) decision rule we get

\begin{align}
\hat{y} = \underset{k \in \{1,\dots,K\}}{\rm{argmax}} \hspace{2mm} p(C_k)\prod^n_{i=1}p(x_i|C_k).
\end{align}

\noindent
We will compare the performance of two Na\"{i}ve Bayes event models, Bernoulli and Multinomial, on our Twitter classification problem.

\subsection{Bernoulli event model}

In the Bernoulli event model we represent tweets as binary vectors, so each word  $x_i \in \{0,1\}$ and $|\vec{x}| = |V|$. Thus, $x_i = 1$ means that the word $V_i$ is contained in the tweet at least once, and $x_i = 0$ means that word $V_i$ is not in the tweet $\vec{x}$. In the Bernoulli model we use

\begin{align}
p(\vec{x}|C) = \prod^{|V|}_{i=1} p_{x_i|C}^{x_i}(1-p_{x_i|C})^{(1-x_i)}
\end{align}

\noindent
to compute the likelihood that a given tweet $\vec{x}$ is from account $C$. And the likelihood that word $x_i$ is found in class $C_k$ is estimated by

\begin{align}
p_{x_i|C_k} = \frac{n_k(x_i)}{N_k},
\end{align}

\noindent
where $n_k(x_i)$ denotes the number of occurrences of word $x_i$ in class $k$ and $N_k$ is the number of tweets from class $k$. Since the product of many probabilities could result in underflow, we compute the log of the likelihoods by

\begin{align}
\mathrm{log} \hspace{1mm} p(\vec{x}|C) &= \mathrm{log}\Big(\prod^{|V|}_{i=1} p_{x_i|C}^{x_i} (1-p_{x_i|C})^{(1-x_i)}\Big) \\
                     &= \sum_{i=1}^{|V|} \lbrack x_i \cdot \mathrm{log} \hspace{1mm} p_{x_i|C} + (1-x_i)\mathrm{log}(1-p_{x_i|C}) \rbrack.
\end{align}

\noindent
Combining equations (1) and (7) we perform MAP estimation in log space with

\begin{align}
\hat{y} = \underset{k \in \{1,\dots,K\}}{\rm{argmax}} \hspace{2mm} \mathrm{log}\hspace{1mm} p(C_k)\sum_{i=1}^{|V|} \lbrack x_i \cdot \mathrm{log} \hspace{1mm} p_{x_i|C_k} + (1-x_i)\mathrm{log}(1-p_{x_i|C_k}) \rbrack.
\end{align}

\noindent
The priors for each class are calculated using Maximum Likelihood Estimation

\begin{align}
p(C_k) = \frac{N_k}{N},
\end{align}

\noindent
where $N$ is the total number of tweets in both classes.

\subsection{Multinomial event model}

In the Multinomial case we represent tweets as integer vectors, so each word $x_i \in \mathbb{Z}$, where each element $x_i$ denotes the frequency of that word in the tweet. The likelihood is given by

\begin{align}
p(\vec{x}|C) = \prod^{|V|}_{i=1} p_{x_i|C}^{x_i},
\end{align}

\noindent
and the probability $p_{x_i|C_k}$ is the relative frequency of word $x_i$ in tweets of class $k$ divided by the total number of words in the tweets of that class. In log space we get


\begin{align}
\mathrm{log} \hspace{1mm} p(\vec{x}|C) &= \mathrm{log}\Big(\prod^{|V|}_{i=1} p_{x_i|C}^{x_i}\Big) \\
                     &= \sum_{i=1}^{|V|} \lbrack x_i \cdot \mathrm{log} \hspace{1mm} p_{x_i|C} \rbrack.
\end{align}

\noindent
Finally, combining equations (1) and (12) we perform MAP estimation with

\begin{align}
\hat{y} = \underset{k \in \{1,\dots,K\}}{\rm{argmax}} \hspace{2mm} \mathrm{log}\hspace{1mm} p(C_k)\sum_{i=1}^{|V|} \lbrack x_i \cdot \mathrm{log} \hspace{1mm} p_{x_i|C_k} \rbrack,
\end{align}

\noindent
where the priors are calculated using equation (9).

\section{Implementation}

\begin{enumerate}

\item [1.] (5 pts) Please explain how you use the log of probability to perform classification

\item [2.] (5 pts) Report the overall testing accuracy (number of correctly classified documents over the toatl number of documents) for both ( Bernoulli and Multinomial) models.

\item [3.] (5 pts) Whose tweets were confused more often than the other? Why do you think this is?

\item [4.] (5 pts) Identify, for each class, the top ten words that have the highest probability.

\end{enumerate}

\subsection{Priors and overfitting (20 pts)}

\subsection{Identifying important features (20 pts)}

\subsection{Bonus}

\end{document}
